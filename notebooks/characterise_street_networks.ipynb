{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install networkx\n",
    "# %pip install osmnx\n",
    "# %pip install scipy\n",
    "# %pip install seaborn\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "ox.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graph files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of capitals of the comarques of Catalonia\n",
    "capitals_comarca = [\n",
    "    \"Barcelona\",\n",
    "    \"Girona\",\n",
    "    \"Lleida\",\n",
    "    \"Tarragona\",\n",
    "    \"Mataró\",\n",
    "    \"Sabadell\",\n",
    "    \"Terrassa\",\n",
    "    \"Manresa\",\n",
    "    \"Vic\",\n",
    "    \"Igualada\",\n",
    "    \"Vilafranca del Penedès\",\n",
    "    \"Vilanova i la Geltrú\",\n",
    "    \"el Vendrell\",\n",
    "    \"Reus\",\n",
    "    \"Tortosa\",\n",
    "    \"Amposta\",\n",
    "    \"Gandesa\",\n",
    "    \"Falset\",\n",
    "    \"Montblanc\",\n",
    "    \"Valls\",\n",
    "    \"Balaguer\",\n",
    "    \"Cervera\",\n",
    "    \"Solsona\",\n",
    "    \"la Seu d'Urgell\",\n",
    "    \"Sort\",\n",
    "    \"Tremp\",\n",
    "    \"el Pont de Suert\",\n",
    "    \"Mollerussa\",\n",
    "    \"les Borges Blanques\",\n",
    "    \"Tàrrega\",\n",
    "    \"Olot\",\n",
    "    \"Ripoll\",\n",
    "    \"Puigcerdà\",\n",
    "    \"Banyoles\",\n",
    "    \"la Bisbal d'Empordà\",\n",
    "    \"Santa Coloma de Farners\",\n",
    "    \"Figueres\",\n",
    "    \"Vielha e Mijaran\",\n",
    "    \"Berga\",\n",
    "    \"Sant Feliu de Llobregat\",\n",
    "    \"Granollers\",\n",
    "    \"Móra d'Ebre\",\n",
    "    \"Prats de Lluçanès\",\n",
    "    \"Moià\",\n",
    "]\n",
    "\n",
    "# Create a directory to save the graphs if it doesn't exist\n",
    "os.makedirs(\"../data/raw/graphs\", exist_ok=True)\n",
    "# Loop through the capitals\n",
    "for capital in capitals_comarca:\n",
    "    # If capital is not saved, download it\n",
    "    if not os.path.exists(f\"../data/raw/graphs/{capital}.graphml\"):\n",
    "        print(f\"Creating graph for {capital}...\")\n",
    "        try:\n",
    "            # Obtain the city graph\n",
    "            G = ox.graph_from_place(\n",
    "                capital + \", Catalunya, Spain\", network_type=\"drive\"\n",
    "            )\n",
    "\n",
    "            # Save it to a file\n",
    "            ox.save_graphml(G, filepath=f\"../data/raw/graphs/{capital}.graphml\")\n",
    "            print(f\"Graph for {capital} created and saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create graph for {capital}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "H_max = math.log(36)  # ~3.584913\n",
    "H_g = 1.386\n",
    "\n",
    "for capital in capitals_comarca:\n",
    "    # If metrics have not been calculated yet\n",
    "    if not os.path.exists(\"../data/processed/network_metrics/network_metrics.csv\"):\n",
    "        try:\n",
    "            # Path to the graphml file for this capital\n",
    "            graph_file = f\"../data/raw/graphs/{capital}.graphml\"\n",
    "            G = ox.load_graphml(graph_file)\n",
    "\n",
    "            # Add edge bearings\n",
    "            ox.bearing.add_edge_bearings(G)\n",
    "\n",
    "            G_proj = ox.projection.project_graph(G)\n",
    "            # Show some basic stats about the network\n",
    "            basic_stats = ox.stats.basic_stats(G_proj)\n",
    "\n",
    "            # Calculate the metrics\n",
    "            num_nodes = len(G.nodes)\n",
    "            num_edges = len(G.edges)\n",
    "            avg_node_degree = sum(dict(G.degree()).values()) / num_nodes\n",
    "            total_length = basic_stats[\"street_length_total\"]\n",
    "            avg_street_length = basic_stats[\"street_length_avg\"]\n",
    "            avg_betweenness_centrality_nodes = nx.betweenness_centrality(G_proj)\n",
    "            avg_betweenness_centrality_nodes = (\n",
    "                sum(avg_betweenness_centrality_nodes.values()) / num_nodes\n",
    "            )\n",
    "            avg_betweenness_centrality_edges = nx.edge_betweenness_centrality(G_proj)\n",
    "            avg_betweenness_centrality_edges = (\n",
    "                sum(avg_betweenness_centrality_edges.values()) / num_edges\n",
    "            )\n",
    "            orientation_entropy = ox.bearing.orientation_entropy(\n",
    "                ox.convert.to_undirected(G)\n",
    "            )\n",
    "            phi = 1.0 - ((orientation_entropy - H_g) / (H_max - H_g)) ** 2\n",
    "            dead_ends = basic_stats[\"streets_per_node_proportions\"][1]\n",
    "            k4_intersections = basic_stats[\"streets_per_node_proportions\"][4]\n",
    "            detour_index = basic_stats[\"circuity_avg\"]\n",
    "\n",
    "            # Add metrics to the table\n",
    "            data.append(\n",
    "                {\n",
    "                    \"Capital\": capital,\n",
    "                    \"Number of Nodes\": num_nodes,\n",
    "                    \"Average Node Degree\": avg_node_degree,\n",
    "                    \"Total Length (m)\": total_length,\n",
    "                    \"Average Street Length (m)\": avg_street_length,\n",
    "                    \"Average Betweenness Centrality (Nodes)\": avg_betweenness_centrality_nodes,\n",
    "                    \"Average Betweenness Centrality (Edges)\": avg_betweenness_centrality_edges,\n",
    "                    \"Orientation Entropy\": orientation_entropy,\n",
    "                    \"Normalized Measure of Orientation-Order\": phi,\n",
    "                    \"Proportion of Dead-ends\": dead_ends,\n",
    "                    \"Proportion of k=4 Intersections\": k4_intersections,\n",
    "                    \"Detour Index\": detour_index,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(f\"Metrics from {capital} calculated and added to the table.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate metrics for {capital}: {e}\")\n",
    "\n",
    "        # Create the DataFrame\n",
    "        df_data = pd.DataFrame(data)\n",
    "        # Create folder for metrics if it doesn't exist\n",
    "        os.makedirs(\"../data/processed/network_metrics\", exist_ok=True)\n",
    "        # Write the dataframe to a csv file\n",
    "        df_data.to_csv(\"../data/processed/network_metrics/network_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the csv file\n",
    "df_data = pd.read_csv(\"../data/processed/network_metrics/network_metrics.csv\")\n",
    "\n",
    "# Add a new column to the DataFrame\n",
    "df_data[\"Orientation Entropy (normalisation 1)\"] = (\n",
    "    df_data[\"Orientation Entropy\"] - H_g\n",
    ") / (H_max - H_g)\n",
    "df_data[\"Orientation Entropy (normalisation 2)\"] = (\n",
    "    (df_data[\"Orientation Entropy\"] - H_g) / (H_max - H_g)\n",
    ") ** 2\n",
    "\n",
    "# Create new dataframe with only the city name and the new column\n",
    "df_entropy = df_data[\n",
    "    [\n",
    "        \"Capital\",\n",
    "        \"Orientation Entropy (normalisation 1)\",\n",
    "        \"Orientation Entropy (normalisation 2)\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create folder for latex tables if it doesn't exist\n",
    "os.makedirs(\"../data/processed/latex\", exist_ok=True)\n",
    "\n",
    "# Export to a latex table\n",
    "df_entropy.to_latex(\"../data/processed/latex/orientation_entropy.tex\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betweenness centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output folder path\n",
    "output_folder = os.path.join(\"../visualisations\", \"betweenness_centrality\", \"edges\")\n",
    "\n",
    "# Create the folder (and any intermediate folders) if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for capital in capitals_comarca:\n",
    "    try:\n",
    "        # Path to the graphml file for this capital\n",
    "        graph_file = f\"../data/raw/graphs/{capital}.graphml\"\n",
    "        G = ox.load_graphml(graph_file)\n",
    "        G_proj = ox.projection.project_graph(G)\n",
    "        # Edge betweenness centrality\n",
    "        edge_betweenness = nx.edge_betweenness_centrality(\n",
    "            G_proj, normalized=True, weight=\"length\"\n",
    "        )\n",
    "        all_edges = list(G_proj.edges(keys=True))\n",
    "        edge_betweenness_values = [\n",
    "            edge_betweenness[(u, v, k)] for (u, v, k) in all_edges\n",
    "        ]\n",
    "\n",
    "        # Normalize betweenness for linewidth scaling\n",
    "        max_bet = max(edge_betweenness_values)\n",
    "        min_bet = min(edge_betweenness_values)\n",
    "        edge_widths = [\n",
    "            1 + 4 * ((bet - min_bet) / (max_bet - min_bet))\n",
    "            for bet in edge_betweenness_values\n",
    "        ]\n",
    "\n",
    "        # Plot the graph with varying edge widths\n",
    "        fig, ax = ox.plot_graph(\n",
    "            G_proj,\n",
    "            edge_color=\"steelblue\",\n",
    "            edge_linewidth=edge_widths,\n",
    "            node_size=0,  # Hide nodes for clarity\n",
    "            bgcolor=\"white\",\n",
    "            show=False,\n",
    "            close=False,\n",
    "        )\n",
    "\n",
    "        # Build the file path for saving\n",
    "        output_path = os.path.join(output_folder, f\"{capital}_edge_betweenness.png\")\n",
    "\n",
    "        # Save the figure at high resolution\n",
    "        fig.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "        # Close the figure to free memory (especially important in a loop)\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Plot saved for {capital} at {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"The visualisation could not be created for {capital}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street angle orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single polar histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output folder path\n",
    "output_folder = os.path.join(\n",
    "    \"../visualisations\", \"polar_histograms\"\n",
    ")\n",
    "\n",
    "# Create the folder (and any intermediate folders) if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for capital in capitals_comarca:\n",
    "    try:\n",
    "        # Path to the graphml file for this capital\n",
    "        graph_file = f\"../data/raw/graphs/{capital}.graphml\"\n",
    "        G = ox.load_graphml(graph_file)\n",
    "\n",
    "        # Ensure graph is undirected\n",
    "        if G.is_directed():\n",
    "            G = ox.convert.to_undirected(G)\n",
    "\n",
    "        # Add bearing attributes to each edge\n",
    "        G = ox.bearing.add_edge_bearings(G)\n",
    "\n",
    "        # Step 2: Extract and duplicate bearings for undirected streets\n",
    "        all_bearings = []\n",
    "        for u, v, key, data in G.edges(keys=True, data=True):\n",
    "            if u == v or \"bearing\" not in data:\n",
    "                continue\n",
    "            bearing = data[\"bearing\"]\n",
    "            all_bearings.append(bearing)\n",
    "            # Add reciprocal bearing for the opposite direction\n",
    "            reverse_bearing = (bearing + 180) % 360\n",
    "            all_bearings.append(reverse_bearing)\n",
    "\n",
    "        bearings = np.array(all_bearings)\n",
    "\n",
    "        # For this example, assume equal weight for each bearing\n",
    "        weights = np.ones_like(bearings)\n",
    "\n",
    "        # Step 3: Use split-and-merge approach to compute histogram bins\n",
    "        num_bins = 36\n",
    "        num_split_bins = num_bins * 2  # 72 bins for splitting\n",
    "        # Create split bin edges from 0° to 360° (for 72 bins)\n",
    "        split_bin_edges = np.linspace(0, 360, num_split_bins + 1)\n",
    "\n",
    "        # Compute histogram on the split bins\n",
    "        split_bin_counts, split_bin_edges = np.histogram(\n",
    "            bearings, bins=split_bin_edges, weights=weights, density=True\n",
    "        )\n",
    "\n",
    "        # Roll the counts so near-boundary bearings are grouped together\n",
    "        split_bin_counts = np.roll(split_bin_counts, 1)\n",
    "\n",
    "        # Merge adjacent bins to form final 36 bins\n",
    "        bin_counts = split_bin_counts[::2] + split_bin_counts[1::2]\n",
    "\n",
    "        # Compute bin centers: every other edge from the split bins\n",
    "        bin_centers = split_bin_edges[range(0, num_split_bins, 2)]\n",
    "\n",
    "        # Convert bin centers to radians for polar plotting\n",
    "        bin_centers_rad = np.radians(bin_centers)\n",
    "\n",
    "        # Width of each bin in radians (10° width)\n",
    "        bin_width_rad = np.radians(10)\n",
    "\n",
    "        # Step 4: Plot the polar histogram\n",
    "        fig, ax = plt.subplots(subplot_kw={\"projection\": \"polar\"}, figsize=(8, 8))\n",
    "\n",
    "        # Create bars for each bin\n",
    "        bars = ax.bar(\n",
    "            bin_centers_rad,\n",
    "            bin_counts,\n",
    "            width=bin_width_rad,\n",
    "            bottom=0.0,\n",
    "            alpha=1,\n",
    "            color=\"steelblue\",\n",
    "        )\n",
    "\n",
    "        # Customize the polar plot\n",
    "        ax.set_theta_zero_location(\"N\")  # Set 0° at the top (North)\n",
    "        ax.set_theta_direction(-1)  # Angles increase clockwise\n",
    "\n",
    "        # Make radial lines at every 45°:\n",
    "        angles = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "\n",
    "        # Give labels for only the cardinal directions, leaving others blank:\n",
    "        labels = [\"N\", \"\", \"E\", \"\", \"S\", \"\", \"W\", \"\"]\n",
    "\n",
    "        ax.set_thetagrids(angles, labels=labels)\n",
    "        ax.set_title(\"Amposta\", y=1.08, fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "        # ──────────────────────────────────────────────\n",
    "        #    MAKE CIRCLES THINNER AND NUMBERS LIGHTER\n",
    "        # ──────────────────────────────────────────────\n",
    "\n",
    "        # 1) Make radial grid lines thinner and a light color\n",
    "        for circle in ax.yaxis.get_gridlines():\n",
    "            circle.set_linewidth(0.5)\n",
    "            circle.set_color(\"lightgray\")\n",
    "\n",
    "        # 2) Make the radial tick labels light gray\n",
    "        for label in ax.yaxis.get_ticklabels():\n",
    "            label.set_color(\"lightgray\")\n",
    "            label.set_fontsize(8)\n",
    "\n",
    "        # 3) Do the same for the radial spine (outer circle):\n",
    "        ax.spines[\"polar\"].set_linewidth(1.0)\n",
    "        ax.spines[\"polar\"].set_color(\"black\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"The visualisation could not be created for {capital}: {e}\")\n",
    "    \n",
    "    # Build the file path for saving\n",
    "    output_path = os.path.join(output_folder, f\"{capital}_polar_histogram.png\")\n",
    "    # Save the figure at high resolution\n",
    "    fig.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    # Close the figure to free memory\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined polar histogram ordered by increasing entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output folder path\n",
    "output_folder = os.path.join(\"../visualisations\", \"polar_histograms\")\n",
    "\n",
    "# Create the folder (and any intermediate folders) if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 1. Sort df_data by increasing Orientation Entropy\n",
    "df_data_sorted = df_data.sort_values(by=\"Orientation Entropy\", ascending=True)\n",
    "\n",
    "# 2. Decide how many subplots across (ncols) and down (nrows) you want\n",
    "#    Since you have 44 cities, you can do, e.g., 6 rows x 8 columns = 48 subplots total\n",
    "nrows, ncols = 6, 8\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, subplot_kw={\"projection\": \"polar\"}, figsize=(24, 18)\n",
    ")  # Feel free to adjust size\n",
    "\n",
    "# 3. Iterate through each city in ascending order of entropy\n",
    "for i, row in enumerate(df_data_sorted.itertuples(index=False)):\n",
    "    capital = row.Capital  # or however you store city names\n",
    "\n",
    "    # Identify which \"ax\" (subplot) we should draw on\n",
    "    ax = axes[i // ncols, i % ncols]\n",
    "\n",
    "    try:\n",
    "        # Load the graph\n",
    "        graph_file = f\"../data/raw/graphs/{capital}.graphml\"\n",
    "        G = ox.load_graphml(graph_file)\n",
    "\n",
    "        # Ensure graph is undirected\n",
    "        if G.is_directed():\n",
    "            G = ox.convert.to_undirected(G)\n",
    "\n",
    "        # Add bearing attributes\n",
    "        G = ox.bearing.add_edge_bearings(G)\n",
    "\n",
    "        # Gather bearings (duplicating for undirected streets)\n",
    "        all_bearings = []\n",
    "        for u, v, key, data in G.edges(keys=True, data=True):\n",
    "            if u == v or \"bearing\" not in data:\n",
    "                continue\n",
    "            bearing = data[\"bearing\"]\n",
    "            all_bearings.append(bearing)\n",
    "            reverse_bearing = (bearing + 180) % 360\n",
    "            all_bearings.append(reverse_bearing)\n",
    "\n",
    "        bearings = np.array(all_bearings)\n",
    "        weights = np.ones_like(bearings)  # Equal weight for each bearing\n",
    "\n",
    "        # Split-and-merge approach\n",
    "        num_bins = 36\n",
    "        num_split_bins = num_bins * 2  # 72 bins\n",
    "        split_bin_edges = np.linspace(0, 360, num_split_bins + 1)\n",
    "\n",
    "        split_bin_counts, _ = np.histogram(\n",
    "            bearings, bins=split_bin_edges, weights=weights, density=True\n",
    "        )\n",
    "        # Roll and merge\n",
    "        split_bin_counts = np.roll(split_bin_counts, 1)\n",
    "        bin_counts = split_bin_counts[::2] + split_bin_counts[1::2]\n",
    "\n",
    "        # Convert bin centers to radians\n",
    "        bin_centers = split_bin_edges[range(0, num_split_bins, 2)]\n",
    "        bin_centers_rad = np.radians(bin_centers)\n",
    "        bin_width_rad = np.radians(10)  # Each bin is 10°\n",
    "\n",
    "        # Plot polar histogram in the current subplot\n",
    "        bars = ax.bar(\n",
    "            bin_centers_rad,\n",
    "            bin_counts,\n",
    "            width=bin_width_rad,\n",
    "            bottom=0.0,\n",
    "            alpha=1,\n",
    "            color=\"steelblue\",\n",
    "        )\n",
    "\n",
    "        # Customize polar orientation\n",
    "        ax.set_theta_zero_location(\"N\")\n",
    "        ax.set_theta_direction(-1)\n",
    "\n",
    "        # Minor aesthetic settings\n",
    "        angles = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "        labels = [\"N\", \"\", \"E\", \"\", \"S\", \"\", \"W\", \"\"]\n",
    "        ax.set_thetagrids(angles, labels=labels)\n",
    "\n",
    "        # Give each subplot a title\n",
    "        # Showing the city name + entropy, if desired\n",
    "        ax.set_title(f\"{capital}\", y=1.15, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        # Make circles thinner and numbers lighter\n",
    "        for circle in ax.yaxis.get_gridlines():\n",
    "            circle.set_linewidth(0.5)\n",
    "            circle.set_color(\"lightgray\")\n",
    "        for label in ax.yaxis.get_ticklabels():\n",
    "            label.set_color(\"lightgray\")\n",
    "            label.set_fontsize(8)\n",
    "        ax.spines[\"polar\"].set_linewidth(1.0)\n",
    "        ax.spines[\"polar\"].set_color(\"black\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # If something fails for a particular city, just print the error\n",
    "        print(f\"Visualization could not be created for {capital}: {e}\")\n",
    "        # Optionally turn off that subplot\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "# 4. Turn off any leftover subplots (if you have fewer than nrows*ncols)\n",
    "total_plots = nrows * ncols\n",
    "for j in range(len(df_data_sorted), total_plots):\n",
    "    axes[j // ncols, j % ncols].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 5. Finally, save the entire figure to the output folder\n",
    "output_path = os.path.join(output_folder, \"polar_histograms_by_increasing_entropy.png\")\n",
    "plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"Saved combined figure with all histograms to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Street network + polar histogram for maximum and minimun entropy cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output folder path\n",
    "output_folder = os.path.join(\"../visualisations\", \"polar_histograms\")\n",
    "\n",
    "# Create the folder (and any intermediate folders) if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "cities = [\"Granollers\", \"Manresa\"]\n",
    "\n",
    "for city_name in cities:\n",
    "    graph_file = f\"../data/raw/graphs/{city_name}.graphml\"\n",
    "\n",
    "    # Load and ensure undirected\n",
    "    G = ox.load_graphml(graph_file)\n",
    "    if G.is_directed():\n",
    "        G = ox.convert.to_undirected(G)\n",
    "\n",
    "    ###############################################################################\n",
    "    # 1) Create a SINGLE main figure + main axes for the street network\n",
    "    ###############################################################################\n",
    "    fig, ax_main = plt.subplots(figsize=(10, 10))  # adjust as you like\n",
    "\n",
    "    # Plot the street network on ax_main\n",
    "    ox.plot_graph(\n",
    "        G,\n",
    "        ax=ax_main,\n",
    "        show=False,\n",
    "        close=False,\n",
    "        node_size=0,\n",
    "        edge_color=\"darkgray\",\n",
    "        edge_linewidth=0.6,\n",
    "    )\n",
    "\n",
    "    # Remove any title, ticks, or labels from the main axes\n",
    "    ax_main.set_title(\"\")  # no title\n",
    "    ax_main.set_xticks([])\n",
    "    ax_main.set_yticks([])\n",
    "\n",
    "    ###############################################################################\n",
    "    # 2) Add a smaller INSET axes for the polar histogram (bottom-right corner)\n",
    "    ###############################################################################\n",
    "    # The [left, bottom, width, height] coordinates range from 0 to 1\n",
    "    # across the figure. Tweak them until the histogram is where you want it.\n",
    "    # Decide inset placement before creating the axes\n",
    "\n",
    "    if city_name == \"Manresa\":\n",
    "        # bottom-left\n",
    "        inset_bounds = [0.1, 0.1, 0.2, 0.2]  # [left, bottom, width, height]\n",
    "    else:\n",
    "        # bottom-right\n",
    "        inset_bounds = [0.65, 0.1, 0.2, 0.2]\n",
    "\n",
    "    inset_ax = fig.add_axes(inset_bounds, projection=\"polar\")\n",
    "\n",
    "    # Next, create the same polar histogram logic as before\n",
    "    G = ox.bearing.add_edge_bearings(G)\n",
    "\n",
    "    all_bearings = []\n",
    "    for u, v, key, data in G.edges(keys=True, data=True):\n",
    "        if u == v or \"bearing\" not in data:\n",
    "            continue\n",
    "        bearing = data[\"bearing\"]\n",
    "        all_bearings.append(bearing)\n",
    "        reverse_bearing = (bearing + 180) % 360\n",
    "        all_bearings.append(reverse_bearing)\n",
    "\n",
    "    bearings = np.array(all_bearings)\n",
    "    weights = np.ones_like(bearings)\n",
    "\n",
    "    num_bins = 36\n",
    "    num_split_bins = num_bins * 2\n",
    "    split_bin_edges = np.linspace(0, 360, num_split_bins + 1)\n",
    "    split_bin_counts, _ = np.histogram(\n",
    "        bearings, bins=split_bin_edges, weights=weights, density=True\n",
    "    )\n",
    "    # Roll, then merge adjacent bins\n",
    "    split_bin_counts = np.roll(split_bin_counts, 1)\n",
    "    bin_counts = split_bin_counts[::2] + split_bin_counts[1::2]\n",
    "\n",
    "    bin_centers = split_bin_edges[range(0, num_split_bins, 2)]\n",
    "    bin_centers_rad = np.radians(bin_centers)\n",
    "    bin_width_rad = np.radians(10)\n",
    "\n",
    "    bars = inset_ax.bar(\n",
    "        bin_centers_rad,\n",
    "        bin_counts,\n",
    "        width=bin_width_rad,\n",
    "        bottom=0.0,\n",
    "        alpha=1,\n",
    "        color=\"steelblue\",\n",
    "    )\n",
    "\n",
    "    # Customize orientation\n",
    "    inset_ax.set_theta_zero_location(\"N\")\n",
    "    inset_ax.set_theta_direction(-1)\n",
    "\n",
    "    # Show radial lines every 45°, label only N/E/S/W\n",
    "    angles = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "    labels = [\"N\", \"\", \"E\", \"\", \"S\", \"\", \"W\", \"\"]\n",
    "    inset_ax.set_thetagrids(angles, labels=labels)\n",
    "\n",
    "    # City name on top of the histogram\n",
    "    inset_ax.set_title(city_name, y=1.15, fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Make circles thinner & radial labels lighter\n",
    "    for circle in inset_ax.yaxis.get_gridlines():\n",
    "        circle.set_linewidth(0.5)\n",
    "        circle.set_color(\"lightgray\")\n",
    "\n",
    "    for label in inset_ax.yaxis.get_ticklabels():\n",
    "        label.set_color(\"lightgray\")\n",
    "        label.set_fontsize(8)\n",
    "\n",
    "    inset_ax.spines[\"polar\"].set_linewidth(1.0)\n",
    "    inset_ax.spines[\"polar\"].set_color(\"black\")\n",
    "\n",
    "    ###############################################################################\n",
    "    # 3) Save the combined figure to the output folder\n",
    "    ###############################################################################\n",
    "    out_path = os.path.join(\n",
    "        output_folder, city_name + \"_street_network_+_polar_histogram.png\"\n",
    "    )\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output folder path\n",
    "output_folder = os.path.join(\"../data/processed/correlations\")\n",
    "\n",
    "# Create the folder (and any intermediate folders) if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df_data = pd.read_csv(\"../data/processed/network_metrics/network_metrics.csv\")\n",
    "\n",
    "# 1) Drop the \"Capital\" column\n",
    "df_numeric = df_data.drop(columns=\"Capital\")\n",
    "\n",
    "# 2) Prepare empty DataFrames to store correlation and p-values\n",
    "corr_vals = pd.DataFrame(\n",
    "    np.zeros((df_numeric.shape[1], df_numeric.shape[1])),\n",
    "    columns=df_numeric.columns,\n",
    "    index=df_numeric.columns,\n",
    ")\n",
    "p_vals = pd.DataFrame(\n",
    "    np.zeros((df_numeric.shape[1], df_numeric.shape[1])),\n",
    "    columns=df_numeric.columns,\n",
    "    index=df_numeric.columns,\n",
    ")\n",
    "\n",
    "# 3) Loop through each pair of columns\n",
    "for col1 in df_numeric.columns:\n",
    "    for col2 in df_numeric.columns:\n",
    "        if col1 == col2:\n",
    "            # The correlation of a column with itself is 1, and p-value is 0 (by definition)\n",
    "            corr_vals.loc[col1, col2] = 1.0\n",
    "            p_vals.loc[col1, col2] = 0.0\n",
    "        else:\n",
    "            # Compute Pearson correlation and p-value\n",
    "            r, p = pearsonr(df_numeric[col1], df_numeric[col2])\n",
    "            corr_vals.loc[col1, col2] = r\n",
    "            p_vals.loc[col1, col2] = p\n",
    "\n",
    "# Save to a CSV file in the output_folder both correlation and p-values\n",
    "corr_vals.to_csv(os.path.join(output_folder, \"correlation_matrix.csv\"))\n",
    "p_vals.to_csv(os.path.join(output_folder, \"p_values_matrix.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep correlation values only if p < 0.05; set everything else to 0\n",
    "corr_significant = corr_vals.where(p_vals < 0.05, np.nan)\n",
    "\n",
    "# Keep p-values only if p < 0.05; set everything else to NaN\n",
    "p_significant = p_vals.where(p_vals < 0.05, np.nan)\n",
    "\n",
    "# Optionally save them to CSV if desired\n",
    "corr_significant.to_csv(os.path.join(output_folder, \"corr_significant.csv\"))\n",
    "p_significant.to_csv(os.path.join(output_folder, \"p_significant.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the correlation matrix\n",
    "corr_vals = pd.read_csv(\"../data/processed/correlations/correlation_matrix.csv\", index_col=0)\n",
    "\n",
    "# Compute the absolute correlation values for coloring\n",
    "abs_corr_vals = corr_vals.abs()\n",
    "\n",
    "# Create a custom colormap using the specified palette\n",
    "custom_cmap = sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True)\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "# use a sequential palette\n",
    "sns.heatmap(\n",
    "    abs_corr_vals,\n",
    "    annot=corr_vals,\n",
    "    fmt=\".2f\",\n",
    "    cmap=custom_cmap,\n",
    "    cbar_kws={\"label\": \"Absolute Correlation\"},\n",
    ")\n",
    "plt.title(\"Correlation Matrix of Network Metrics\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"../visualisations/correlations\", exist_ok=True)\n",
    "\n",
    "# Save the plot\n",
    "output_path = os.path.join(\"../visualisations/correlations\", \"correlation_matrix.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pearson p-values matrix\n",
    "p_vals = pd.read_csv(\"../data/processed/correlations/p_values_matrix.csv\", index_col=0)\n",
    "\n",
    "# Define two colors: one for p-values <= 0.05, one for p-values > 0.05\n",
    "# Choose colors as needed; here we use \"lightblue\" for non-significant and \"salmon\" for significant\n",
    "colors = [\"#99ff99\", \"#ff9999\"]\n",
    "\n",
    "# Create a discrete colormap using the specified colors\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Set boundaries for the two categories: [0, 0.05] and (0.05, 1]\n",
    "norm = BoundaryNorm([0, 0.05, 1], ncolors=cmap.N, clip=True)\n",
    "\n",
    "# Plot the p-values matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(p_vals, annot=True, fmt=\".4f\", cmap=cmap, norm=norm, annot_kws={\"size\": 8})\n",
    "plt.title(\"P-Values Matrix of Network Metrics\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "output_path = os.path.join(\"../visualisations/correlations\", \"p_values_matrix.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Province and vegueria aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llista de capitals de comarca amb les seves províncies i vegueries\n",
    "data = {\n",
    "    \"Capital\": [\n",
    "        \"Barcelona\",\n",
    "        \"Girona\",\n",
    "        \"Lleida\",\n",
    "        \"Tarragona\",\n",
    "        \"Mataró\",\n",
    "        \"Sabadell\",\n",
    "        \"Terrassa\",\n",
    "        \"Manresa\",\n",
    "        \"Vic\",\n",
    "        \"Igualada\",\n",
    "        \"Vilafranca del Penedès\",\n",
    "        \"Vilanova i la Geltrú\",\n",
    "        \"el Vendrell\",\n",
    "        \"Reus\",\n",
    "        \"Tortosa\",\n",
    "        \"Amposta\",\n",
    "        \"Gandesa\",\n",
    "        \"Falset\",\n",
    "        \"Montblanc\",\n",
    "        \"Valls\",\n",
    "        \"Balaguer\",\n",
    "        \"Cervera\",\n",
    "        \"Solsona\",\n",
    "        \"la Seu d'Urgell\",\n",
    "        \"Sort\",\n",
    "        \"Tremp\",\n",
    "        \"el Pont de Suert\",\n",
    "        \"Mollerussa\",\n",
    "        \"les Borges Blanques\",\n",
    "        \"Tàrrega\",\n",
    "        \"Olot\",\n",
    "        \"Ripoll\",\n",
    "        \"Puigcerdà\",\n",
    "        \"Banyoles\",\n",
    "        \"la Bisbal d'Empordà\",\n",
    "        \"Santa Coloma de Farners\",\n",
    "        \"Figueres\",\n",
    "        \"Vielha e Mijaran\",\n",
    "        \"Berga\",\n",
    "        \"Sant Feliu de Llobregat\",\n",
    "        \"Granollers\",\n",
    "        \"Móra d'Ebre\",\n",
    "        \"Prats de Lluçanès\",\n",
    "        \"Moià\",\n",
    "    ],\n",
    "    \"Provincia\": [\n",
    "        \"Barcelona\",\n",
    "        \"Girona\",\n",
    "        \"Lleida\",\n",
    "        \"Tarragona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Tarragona\",\n",
    "        \"Tarragona\",\n",
    "        \"Tarragona\",\n",
    "        \"Tarragona\",\n",
    "        \"Tarragona\",\n",
    "        \"Tarragona\",\n",
    "        \"Tarragona\",\n",
    "        \"Tarragona\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Lleida\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Tarragona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "    ],\n",
    "    \"Vegueria\": [\n",
    "        \"Barcelona\",\n",
    "        \"Girona\",\n",
    "        \"Lleida\",\n",
    "        \"Camp de Tarragona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Catalunya Central\",\n",
    "        \"Catalunya Central\",\n",
    "        \"Catalunya Central\",\n",
    "        \"Penedès\",\n",
    "        \"Penedès\",\n",
    "        \"Penedès\",\n",
    "        \"Camp de Tarragona\",\n",
    "        \"Terres de l'Ebre\",\n",
    "        \"Terres de l'Ebre\",\n",
    "        \"Terres de l'Ebre\",\n",
    "        \"Camp de Tarragona\",\n",
    "        \"Camp de Tarragona\",\n",
    "        \"Camp de Tarragona\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Catalunya Central\",\n",
    "        \"Alt Pirineu i Aran\",\n",
    "        \"Alt Pirineu i Aran\",\n",
    "        \"Alt Pirineu i Aran\",\n",
    "        \"Alt Pirineu i Aran\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Lleida\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Alt Pirineu i Aran\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Girona\",\n",
    "        \"Alt Pirineu i Aran\",\n",
    "        \"Catalunya Central\",\n",
    "        \"Barcelona\",\n",
    "        \"Barcelona\",\n",
    "        \"Terres de l'Ebre\",\n",
    "        \"Catalunya Central\",\n",
    "        \"Catalunya Central\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create the DataFrame with vegueria and provincia information\n",
    "df_info = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network_metrics.csv\n",
    "df_metrics = pd.read_csv(\"../data/processed/network_metrics/network_metrics.csv\")\n",
    "\n",
    "# Join on the \"Capital\" column with province and vegueria information\n",
    "df = pd.merge(df_metrics, df_info, on=\"Capital\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the probability density function graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(\"../visualisations/distributions/vegueria\", exist_ok=True)\n",
    "os.makedirs(\"../visualisations/distributions/province\", exist_ok=True)\n",
    "\n",
    "# For all measures create a graph of the probability density function of the phi metric for the capitals of comarca separated by vegueria and province\n",
    "for measure in df.columns[2:12]:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.kdeplot(data=df, x=measure, hue=\"Vegueria\", ax=ax)\n",
    "    ax.set_title(f\"Distribution of {measure} by Vegueria\")\n",
    "    # Save the plot\n",
    "    plt.savefig(f\"../visualisations/distributions/vegueria/{measure}_by_vegueria.png\")\n",
    "    plt.close(fig) \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.kdeplot(data=df, x=measure, hue=\"Provincia\", ax=ax)\n",
    "    ax.set_title(f\"Distribution of {measure} by Province\")\n",
    "    # Save the plot\n",
    "    plt.savefig(f\"../visualisations/distributions/province/{measure}_by_province.png\")\n",
    "    plt.close(fig)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean values of each province and Vegueria so that the values can be displayed in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean of the metrics by vegueria\n",
    "df_vegureia = df.drop(columns=[\"Capital\", \"Provincia\"])\n",
    "df_vegueria = (\n",
    "    df_vegureia.groupby(\n",
    "        \"Vegueria\",\n",
    "    )\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# save it in a csv\n",
    "df_vegueria.to_csv(\"../data/processed/network_metrics/metrics_by_vegueria_mean.csv\", index=False)\n",
    "\n",
    "# calculate mean of the metrics by province\n",
    "df_province = df.drop(columns=[\"Capital\", \"Vegueria\"])\n",
    "df_province = (\n",
    "    df_province.groupby(\n",
    "        \"Provincia\",\n",
    "    )\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# save it in a csv\n",
    "df_province.to_csv(\"../data/processed/network_metrics/metrics_by_province_mean.csv\", index=False)\n",
    "\n",
    "# calculate the median of the metrics by vegueria\n",
    "df_vegureia = df.drop(columns=[\"Capital\", \"Provincia\"])\n",
    "df_vegueria = (\n",
    "    df_vegureia.groupby(\n",
    "        \"Vegueria\",\n",
    "    )\n",
    "    .median()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# save it in a csv\n",
    "df_vegueria.to_csv(\"../data/processed/network_metrics/metrics_by_vegueria_median.csv\", index=False)\n",
    "\n",
    "# calculate the median of the metrics by province\n",
    "df_province = df.drop(columns=[\"Capital\", \"Vegueria\"])\n",
    "df_province = (\n",
    "    df_province.groupby(\n",
    "        \"Provincia\",\n",
    "    )\n",
    "    .median()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# save it in a csv\n",
    "df_province.to_csv(\"../data/processed/network_metrics/metrics_by_province_median.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dendograms with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the metrics.csv file\n",
    "df_metrics = pd.read_csv(\"../data/processed/network_metrics/network_metrics.csv\")\n",
    "\n",
    "# Select the columns to be used for clustering\n",
    "columns_to_cluster = [\n",
    "    \"Number of Nodes\",\n",
    "    \"Number of Edges\",\n",
    "    \"Average Node Degree\",\n",
    "    \"Total Length (m)\",\n",
    "    \"Average Edge Length (m)\",\n",
    "    \"Betweenness Centrality (Nodes)\",\n",
    "    \"Betweenness Centrality (Edges)\",\n",
    "    \"Orientation Entropy\",\n",
    "    \"Proportion of Dead-ends\",\n",
    "    \"Proportion of k=4 Intersections\",\n",
    "    \"Detour Index\",\n",
    "]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_metrics.drop(columns=(\"Capital\")))\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "output_folder = \"../visualisations/clustering_all_vars\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "linkage_methods = [\n",
    "    \"ward\",\n",
    "    \"single\",\n",
    "    \"complete\",\n",
    "    \"average\",\n",
    "    \"centroid\",\n",
    "    \"weighted\",\n",
    "    \"median\",\n",
    "]\n",
    "for method in linkage_methods:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    dendrogram = sch.dendrogram(\n",
    "        sch.linkage(X, method=method), labels=df_metrics[\"Capital\"].values\n",
    "    )\n",
    "    plt.title(f\"Dendrogram with {method} linkage\")\n",
    "    plt.xlabel(\"Capitals\")\n",
    "    plt.ylabel(\"Euclidean distances\")\n",
    "    plt.savefig(os.path.join(output_folder, f\"dendrogram_{method}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the number of clusters of the ward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the metrics.csv file\n",
    "df_metrics = pd.read_csv(\"../data/processed/network_metrics/network_metrics.csv\")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_metrics.drop(columns=(\"Capital\")))\n",
    "\n",
    "# Perform hierarchical/agglomerative clustering using Ward's method\n",
    "Z = linkage(X, method=\"ward\", metric=\"euclidean\")\n",
    "\n",
    "# Elbow Method\n",
    "wcss = []\n",
    "k_range = range(1, 15)\n",
    "for k in k_range:\n",
    "    clusters = fcluster(Z, k, criterion=\"maxclust\")\n",
    "    centroids = np.array([X[clusters == i].mean(axis=0) for i in range(1, k + 1)])\n",
    "    wcss.append(\n",
    "        sum(np.min(np.linalg.norm(X[:, np.newaxis] - centroids, axis=2), axis=1) ** 2)\n",
    "    )\n",
    "\n",
    "# Plot the Elbow Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, wcss, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Within-cluster sum of squares\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.savefig(\"../visualisations/clustering_all_vars/elbow_method.png\")\n",
    "plt.close()\n",
    "\n",
    "# Silhouette Method\n",
    "silhouette_scores = []\n",
    "for k in k_range:\n",
    "    clusters = fcluster(Z, k, criterion=\"maxclust\")\n",
    "    if k > 1:\n",
    "        score = silhouette_score(X, clusters)\n",
    "        silhouette_scores.append(score)\n",
    "    else:\n",
    "        silhouette_scores.append(0)  # Silhouette score is not defined for k=1\n",
    "\n",
    "# Plot the Silhouette Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Method\")\n",
    "plt.savefig(\"../visualisations/clustering_all_vars/silhouette_method.png\")\n",
    "plt.close()\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "davies_bouldin_scores = []\n",
    "for k in k_range:\n",
    "    clusters = fcluster(Z, k, criterion=\"maxclust\")\n",
    "    if k > 1:\n",
    "        score = davies_bouldin_score(X, clusters)\n",
    "        davies_bouldin_scores.append(score)\n",
    "    else:\n",
    "        davies_bouldin_scores.append(\n",
    "            float(\"inf\")\n",
    "        )  # Davies-Bouldin score is not defined for k=1\n",
    "\n",
    "# Plot the Davies-Bouldin Index\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, davies_bouldin_scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Davies-Bouldin Index\")\n",
    "plt.title(\"Davies-Bouldin Index\")\n",
    "plt.savefig(\"../visualisations/clustering_all_vars/davies_bouldin_index.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical/agglomerative clustering using Ward's method\n",
    "Z = linkage(X, method=\"ward\", metric=\"euclidean\")\n",
    "\n",
    "# Choose the number of clusters\n",
    "num_clusters = 5\n",
    "clusters = fcluster(Z, num_clusters, criterion=\"maxclust\")\n",
    "\n",
    "# Add the cluster labels to the dataframe\n",
    "df_metrics[\"Cluster\"] = clusters\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "output_folder = \"../data/processed/clusters\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the dataframe with cluster labels to a CSV file\n",
    "df_metrics.to_csv(f\"{output_folder}/ward_clusters_5_all_vars.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE projection\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Add the t-SNE components to the dataframe\n",
    "df_metrics[\"TSNE1\"] = X_tsne[:, 0]\n",
    "df_metrics[\"TSNE2\"] = X_tsne[:, 1]\n",
    "\n",
    "# Plot the t-SNE projection with clusters and annotate some points\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = sns.scatterplot(\n",
    "    data=df_metrics,\n",
    "    x=\"TSNE1\",\n",
    "    y=\"TSNE2\",\n",
    "    hue=\"Cluster\",\n",
    "    palette=\"tab10\",\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "# Annotate all points with city names\n",
    "for i, point in df_metrics.iterrows():\n",
    "    plt.text(\n",
    "        point[\"TSNE1\"],\n",
    "        point[\"TSNE2\"],\n",
    "        point[\"Capital\"],\n",
    "        horizontalalignment=\"left\",\n",
    "        size=\"medium\",\n",
    "        color=\"black\",\n",
    "        weight=\"semibold\",\n",
    "    )\n",
    "\n",
    "plt.title(\"t-SNE Projection of Clusters\")\n",
    "plt.xlabel(\"TSNE1\")\n",
    "plt.ylabel(\"TSNE2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.savefig(\"../visualisations/clustering_all_vars/tsne_all_vars.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering only with  average node degree, orientation indicator, median street segment length i average circuitry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.read_csv(\"../data/processed/network_metrics/network_metrics.csv\")\n",
    "\n",
    "# Select the features for clustering\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(\n",
    "    df_metrics[\n",
    "        [\n",
    "            \"Average Node Degree\",\n",
    "            \"Normalized Measure of Orientation-Order\",\n",
    "            \"Average Street Length (m)\",\n",
    "            \"Detour Index\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Perform hierarchical/agglomerative clustering using Ward's method\n",
    "Z = linkage(X, method=\"ward\", metric=\"euclidean\")\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z, labels=df_metrics[\"Capital\"].values, leaf_rotation=90, leaf_font_size=10)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Province\")\n",
    "plt.ylabel(\"Distance\")\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"../visualisations/clustering_selected_vars\", exist_ok=True)\n",
    "# Save the dendrogram\n",
    "plt.savefig(\"../visualisations/clustering_selected_vars/dendrogram.png\")\n",
    "plt.show()\n",
    "\n",
    "# Choose the number of clusters\n",
    "max_d = 150  # Adjust this threshold based on the dendrogram\n",
    "clusters = fcluster(Z, max_d, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method\n",
    "wcss = []\n",
    "k_range = range(1, 15)\n",
    "for k in k_range:\n",
    "    clusters = fcluster(Z, k, criterion=\"maxclust\")\n",
    "    centroids = np.array([X[clusters == i].mean(axis=0) for i in range(1, k + 1)])\n",
    "    wcss.append(\n",
    "        sum(np.min(np.linalg.norm(X[:, np.newaxis] - centroids, axis=2), axis=1) ** 2)\n",
    "    )\n",
    "\n",
    "# Plot the Elbow Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, wcss, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Within-cluster sum of squares\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.savefig(\"../visualisations/clustering_selected_vars/elbow_method.png\")\n",
    "plt.close()\n",
    "\n",
    "# Silhouette Method\n",
    "silhouette_scores = []\n",
    "for k in k_range:\n",
    "    clusters = fcluster(Z, k, criterion=\"maxclust\")\n",
    "    if k > 1:\n",
    "        score = silhouette_score(X, clusters)\n",
    "        silhouette_scores.append(score)\n",
    "    else:\n",
    "        silhouette_scores.append(0)  # Silhouette score is not defined for k=1\n",
    "\n",
    "# Plot the Silhouette Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Method\")\n",
    "plt.savefig(\"../visualisations/clustering_selected_vars/silhouette_method.png\")\n",
    "plt.close()\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "davies_bouldin_scores = []\n",
    "for k in k_range:\n",
    "    clusters = fcluster(Z, k, criterion=\"maxclust\")\n",
    "    if k > 1:\n",
    "        score = davies_bouldin_score(X, clusters)\n",
    "        davies_bouldin_scores.append(score)\n",
    "    else:\n",
    "        davies_bouldin_scores.append(\n",
    "            float(\"inf\")\n",
    "        )  # Davies-Bouldin score is not defined for k=1\n",
    "\n",
    "# Plot the Davies-Bouldin Index\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, davies_bouldin_scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Davies-Bouldin Index\")\n",
    "plt.title(\"Davies-Bouldin Index\")\n",
    "plt.savefig(\"../visualisations/clustering_selected_vars/davies_bouldin_index.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical/agglomerative clustering using Ward's method\n",
    "Z = linkage(X, method=\"ward\", metric=\"euclidean\")\n",
    "\n",
    "# Choose the number of clusters\n",
    "num_clusters = 5\n",
    "clusters = fcluster(Z, num_clusters, criterion=\"maxclust\")\n",
    "\n",
    "# Add the cluster labels to the dataframe\n",
    "df_metrics[\"Cluster\"] = clusters\n",
    "\n",
    "# Save the dataframe with cluster labels to a CSV file\n",
    "df_metrics.to_csv(\"../data/clusters/ward_clusters_5_selected_vars.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE projection\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Add the t-SNE components to the dataframe\n",
    "df_metrics[\"TSNE1\"] = X_tsne[:, 0]\n",
    "df_metrics[\"TSNE2\"] = X_tsne[:, 1]\n",
    "\n",
    "# Plot the t-SNE projection with clusters and annotate some points\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = sns.scatterplot(\n",
    "    data=df_metrics,\n",
    "    x=\"TSNE1\",\n",
    "    y=\"TSNE2\",\n",
    "    hue=\"Cluster\",\n",
    "    palette=\"tab10\",\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "# Annotate all points with city names\n",
    "for i, point in df_metrics.iterrows():\n",
    "    plt.text(\n",
    "        point[\"TSNE1\"],\n",
    "        point[\"TSNE2\"],\n",
    "        point[\"Capital\"],\n",
    "        horizontalalignment=\"left\",\n",
    "        size=\"medium\",\n",
    "        color=\"black\",\n",
    "        weight=\"semibold\",\n",
    "    )\n",
    "\n",
    "plt.title(\"t-SNE Projection of Clusters\")\n",
    "plt.xlabel(\"TSNE1\")\n",
    "plt.ylabel(\"TSNE2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.savefig(\"../visualisations/clustering_selected_vars/tsne_selected_vars.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate street segment lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = [\"Barcelona\", \"Girona\", \"Lleida\", \"Tarragona\"]\n",
    "\n",
    "output_folder = \"../data/street_segment_length\"\n",
    "# Create the folder (and any intermediate folders) if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for capital in capitals:\n",
    "    # Load the graph\n",
    "    graph_file = f\"../data/raw/graphs/{capital}.graphml\"\n",
    "    G = ox.load_graphml(graph_file)\n",
    "\n",
    "    G = ox.convert.to_undirected(G)\n",
    "    G_proj = ox.projection.project_graph(G)\n",
    "\n",
    "    # Convert undirected graph to GeoDataFrame of edges\n",
    "    edges_gdf = ox.convert.graph_to_gdfs(G_proj, nodes=False, edges=True)\n",
    "\n",
    "    # Export just the lengths\n",
    "    lengths = edges_gdf[\"length\"]\n",
    "\n",
    "    # Export to the output folder in a single-column CSV with header\n",
    "    output_path = os.path.join(output_folder, f\"{capital}_street_lengths.csv\")\n",
    "    lengths.to_csv(output_path, index=False, header=[\"Length (m)\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
